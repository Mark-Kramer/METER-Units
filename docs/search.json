[
  {
    "objectID": "METER_P_Values-Quarto.html",
    "href": "METER_P_Values-Quarto.html",
    "title": "Putting the p-value in Context",
    "section": "",
    "text": "Neuroscience researchers typically report p-values to express the strength of statistical evidence; but p-values are not sufficient on their own to understand the meaning and value of a scientific inference. In this unit, learners will learn how to interpret the p-value, how to express the size of an effect and uncertainty about a result, and how to interpret results at both the individual and population levels."
  },
  {
    "objectID": "METER_P_Values-Quarto.html#introduction",
    "href": "METER_P_Values-Quarto.html#introduction",
    "title": "Putting the p-value in Context",
    "section": "Introduction",
    "text": "Introduction\n\nYou work in a sleep lab studying the effect of a new treatment regimen on memory consolidation during sleep.\nYour lab collects an EEG biomarker of memory (sleep spindles) from N=20 human subjects.\nTo do so, your lab measures the power in the spindle band (9-15 Hz) twice per minute. Your lab has a reliable method to detect spindle activity; this detector is known to have small measurement errors outside of treatment. You expect it to still work during treatment, but also expect more variability in the spindle power estimates (hence more variability in the detections) during treatment.\nFor each subject, your lab measures spindle activity during three conditions:\n\nBaseline: Data collection lasts 7 hours while the subject sleeps the night before the intervention. This results in 840 samples of spindle activity for each subject.\nDuring Treatment: Data collection during a 15 minute intervention during sleep, resulting in 30 samples of spindle activity for each subject.\nPost-treatment: Data collection after intervention lasts 7 hours, while the subject sleeps, resulting in 840 samples of spindle activity for each subject.\n\n\nHere’s a graphical representation of the data collected from one subject: \nYour PI says: “I hypothesize that some subjects will show an increase in spindle activity as a result of this treatment. Conduct a hypothesis test for each subject to determine if they are responsive and report the p-values associated with each test.”"
  },
  {
    "objectID": "METER_P_Values-Quarto.html#do-we-have-evidence-to-reject-the-null-hypothesis",
    "href": "METER_P_Values-Quarto.html#do-we-have-evidence-to-reject-the-null-hypothesis",
    "title": "Putting the p-value in Context",
    "section": "Do we have evidence to reject the null hypothesis?",
    "text": "Do we have evidence to reject the null hypothesis?\nMaybe … if we had performed one statistical test, then we typically reject the null hypothesis if\np &lt; 0.05\nBut here we compute 20 test (one for each subject).\nWhen we perform multiple tests, it’s important we consider the impact of multiple comparisons. We cover this topic in detail in the Multiplicity Unit.\nHere we’ll chose a specific approach to deal with multiplicity - we’ll apply a Bonferroni correction. The Bonferroni correction reduces the Type I error rate by dividing the desired overall significance level (here 0.05) by the number of tests performed (here 20 tests, one test per subject). Stated simply, the Bonferroni test adjusts the significance level by dividing it by the number of tests we perform. Doing so reduces the risk of false positives (Type I errors); or more information, see Multiplicity Unit.\nSo, for our analysis of the p-values from 20 subjects, let’s compare the p-values to a stricter threshold of\np &lt; 0.05 / 20 or p &lt; 0.0025\nThresholding in this way provides a binary, yes/no answer to the question: do we have evidence that the spindle activity during treatment differs from 0?\nLet’s plot the p-values versus this new threhsold.\n\n\nCode\n# Plot the p-values during treatment\nplt.figure(figsize=(12, 2))\nplt.stem(p_value_during);\nplt.axhline(y=0.05/20, color='r', linestyle='--')\nplt.xlabel('Subject'); plt.ylabel('p value'); plt.title('During Treatment'); plt.yscale('log')\n\nprint('Significant p-values during treatment = ',np.sum(p_value_during &lt; 0.05/20))\n\n\nSignificant p-values during treatment =  0\n\n\n\n\n\n\n\n\n\n\nQ: After the Bonferroni correction, can we reject the null hypothesis for any subject?\nA: (Multiple Choice)\n\n(Correct) No. None of the p-values are less than 0.05/20.\n(Incorrect) Yes. Some of the p-values are small enough.\n\n\n\nQ: The PI requested “Give me the p’s!”. Do you have evidence to reject the null hypothesis during treatment?\nA: (Multiple Choice)\n\n(Correct) No! The p-values are large, so we find no evidence to reject the null hypothesis for any subject.\n(Incorrect) Yes! The p-values are large, so the spindle activity during treatment is large.\n\n\n\nQ: We do not find any p-values that pass our significance threshold during treatment. Does this mean that the spindle activity during treatment does not change relative to baseline?\nA: (Multiple Choice)\n\n(Correct) No! We never accept the null hypothesis. Remember we have to talk like a statistician. Instead, we say: “We fail to reject the null hypothesis that the spindle activity during treatment differs from baseline.”\n(Incorrect) Yes! Because the p-values are large, we can accept the null hypothesis.\n\n\n\nSummary:\nWe’ve done this for each subject. Our intial results suggest provide no evidence that we can reject the null hypothesis during treatment.\n\n\nMini Summary & Review\nWe sought to answer the scientific question: - Does the spindle activity during treatment differ from the baseline spindle activity?\nTo answer this question, we assumed a Null hypothesis of no difference in spindle activity during treatment compared to 0.\nWe tested this null hypothsis for each subject, computing a p-value for each subject.\nBecause we computed 20 p-values (one for each subject), we corrected for multiple comparsions using a Bonferroni correction (see Multiplicity Unit).\nWe found no p-values small enough to reject the null hypothesis.\nIn other words, using our initial approach, we found no evidence that the spindle activity during treatment differ from baseline.\n\nQ: Our initial results provide we found no evidence that the spindle activity during treatment differ from baseline. The spindle activity might differ between the two conditions, but our data and analysis approach don’t provide supporting evidence. What factors might impact our evidence?\nA: (Multiple choice - all correct)\n\nSample Size: We collect only 30 spindle samples during treatment, which increases random error and provides less precise estimates.\nEffect Size: Small differences in spindle activity between conditions are difficult to detect.\n\nVariability (or Noise) in Measurements: High variability in the spindle estimates during treatment can make it harder to detect a real effect.\nApproach to Statistical Testing: A different approach may provide more insight into difference in spindle activity between the treatment and baseline\nTreatment has no Effect: It may be that the treatment does not impact spindle activity relative to baseline."
  },
  {
    "objectID": "METER_P_Values-Quarto.html#but-are-we-sure",
    "href": "METER_P_Values-Quarto.html#but-are-we-sure",
    "title": "Putting the p-value in Context",
    "section": "But are we sure?",
    "text": "But are we sure?\n\nQ: Review the characteristics of data during treatment and post-treatment. How might these characteristics impact the p-values we observe?\nA: (Multiple Choice - all correct)\n\nWe collect more samples post-treatment, which can provide more precise estimates.\nWe collect fewer samples during treatment, which can provide less precise estimates.\nThe measures are less noisy post-treatment, which can make it easier to detect an effect.\n\nThe measures are more noisy during treatment, which can make it harder to detect an effect.\n\n\n\nThis is a very important question … and we haven’t fully answered it yet.\nWe collect many more samples post-treatment, and our measurements are more accurate post-treatment compared to during treatment. Both of these features impact the evidence we collect to reject the null hypothesis."
  },
  {
    "objectID": "METER_P_Values-Quarto.html#so-are-you-sure-about-the-post-treatment-results",
    "href": "METER_P_Values-Quarto.html#so-are-you-sure-about-the-post-treatment-results",
    "title": "Putting the p-value in Context",
    "section": "So, are you sure about the post-treatment results?",
    "text": "So, are you sure about the post-treatment results?\n\nAlert: Wait, I’m not so sure … \n\n\nWhy did you ask me to review the characteristics of the data, and think about how this might impact the data?\n\n\n\nMoment of tension:\n\nHook the learner - “something isn’t right and I want to know why.”\n\n\n\nQ: EXTENSION PROGRAMMING EXERCISE: We’ve examined the spindle activity during treatment and post-treatment. How would you test the null hypothesis of spindle activity = 0 during the baseline condition?\n\n\nMini Summary & Review\nWe sought to answer the scientific question: - Does the spindle activity post-treatment differ from the baseline spindle activity?\nTo answer this question, we assumed a Null hypothesis of no difference in spindle activity post-treatment compared to 0.\nWe tested this null hypothsis for each subject, computing a p-value for each subject.\nBecause we computed 20 p-values (one for each subject), we corrected for multiple comparsions using a Bonferroni correction (see Multiplicity Unit).\nWe found many p-values small enough to reject the null hypothesis.\nIn other words, in this exploratory analysis, we found evidence that the spindle activity during post-treatment differs from baseline.\nThis differs from our results during treatment, in which we found no evidence that the spindle activity during treatment differs from baseline."
  },
  {
    "objectID": "METER_P_Values-Quarto.html#to-resolve-these-confusing-conclusions-lets-think-more-carefully-about-what-the-p-value-represents.",
    "href": "METER_P_Values-Quarto.html#to-resolve-these-confusing-conclusions-lets-think-more-carefully-about-what-the-p-value-represents.",
    "title": "Putting the p-value in Context",
    "section": "To resolve these confusing conclusions, let’s think more carefully about what the p-value represents.",
    "text": "To resolve these confusing conclusions, let’s think more carefully about what the p-value represents.\nThe p-value measures the strength of evidence against the null hypothesis.\nThree factors can impact the strength of evidence:\n\nSample Size (i.e., the number of observations).\nEffect Size (i.e., bigger differences in spindle activity between conditions are easier to detect.)\nVariability (or Noise) in Measurements: (i.e., how reliably we measure spindle activity).\n\n\nQ: How does the sample size differ during treatment versus post-treatment? How might this impact the results?\nA: - We have many more observations post-treatment (N=840). Therefore, we can accumulate enough evidence to detect a weak effect post-treatment. - We have few observations during treatment (N=30). Therefore, even though the effect may be strong, we don’t have enough evidence to reject the null hypothesis of no difference from 0 (i.e., no difference from baseline) during treatment.\n\n\nQ: How does the effect size differ during treatment versus post-treatment? How might this impact the results?\nA: - The effect size appears small post-treatment (the mean values are near zero). Therefore, although we detect a change post-treatment, this change is small. - The effect size appears large during treatment (the mean values exceed zero). However, due to the limited number of samples and variability in the estimates, we do not have enough evidence to reject the null hypothesis of no difference from 0 (i.e., no difference from baseline) during treatment.\n\n\nQ: How does the measurement variability differ during treatment versus post-treatment? How might this impact the results?\nA:\n\nWe have less measurement variability post-treatment. Lower variability makes it easier to detect a difference from 0 (i.e., difference from baseline).\nWe have more measurement variability during treatment . Higher variability makes it harder to detect a difference from 0 (i.e., difference from baseline) and harder to reject the null hypothesis."
  },
  {
    "objectID": "METER_P_Values-Quarto.html#conclusion-summary-morale",
    "href": "METER_P_Values-Quarto.html#conclusion-summary-morale",
    "title": "Putting the p-value in Context",
    "section": "Conclusion / Summary / Morale:",
    "text": "Conclusion / Summary / Morale:\nWe began with the scientific statement:\n“I expect during treatment that spindle activity exceeds the baseline spindle activity.”\nOur initial approach focused on computing and comparing p-values.\nThat’s a bad idea.\nWe’re not interested in comparing the evidence we have for each null-hypothesis (the p-value); the evidence depends on the sample size, effect size, and measurement variability.\nInstead, we’re more interested in comparing the spindle activity between condidtions.\nIn other words, we’re intested in the effect size, not the p-value.\nThis suggests a different analysis path forward for an improved approach.\nWe can answer the same scienfitic question by comparing the spindle activities between conditions, not the p-values.\nWe’ve started to see this in the plots of spindle activity at baseline, during treatment, and post-treatment.\nFor more analysis (e.g., different statistical test and effect size) continue on to the other Minis."
  },
  {
    "objectID": "METER_P_Values-Quarto.html#optional-section-lme",
    "href": "METER_P_Values-Quarto.html#optional-section-lme",
    "title": "Putting the p-value in Context",
    "section": "7.5 - Optional Section: LME",
    "text": "7.5 - Optional Section: LME\nEstimate effect size and responders\nIn Intro: initial H is some people respond and some don’t"
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html",
    "href": "METER_Sample_Size-SHORT-Quarto.html",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "",
    "text": "Based on the groundbreaking research previously conducted in your lab, you and your collaborators have formulated a compelling scientific hypothesis: substance \\(x\\) could be a genetic biomarker for longevity, potentially influencing the age at which individuals pass away. This intriguing hypothesis opens up a new frontier in our understanding of genetics and lifespan, promising significant advancements in the field.\nBefore we can embark on an experimental journey to test the predictive power of this novel biomarker, we must first tackle a critical step: determining the appropriate sample size for a follow-up research study. The sample size is not just a number; it is a cornerstone of experimental design that ensures our data will be robust enough to support or refute our hypothesis.\nTo accurately compute this sample size, we need to consider our prior beliefs and existing knowledge about substance \\(x\\) and its relationship to longevity. Let’s delve into the specifics. Imagine we have the following limited yet crucial pieces of information:\n\nDistribution of Substance \\(x\\): The expression levels of substance \\(x\\) in people follow a normal distribution.\nImpact on Longevity: Individuals at the high end of the expression spectrum tend to live approximately 5 years longer than those at the low end.\n\nGiven these insights, our task is to calculate a sample size that can yield statistically significant results. This endeavor will not only help us test our hypothesis with precision but also pave the way for future research that could revolutionize our understanding of genetic influences on lifespan. We will see that the sample size required to generate data that can support a scientific hypothesis depends directly on the prior beliefs and knowledge about that hypothesis. Let’s proceed with this vital calculation, knowing that the outcomes will bring us one step closer to potentially groundbreaking discoveries in genetic biomarkers and longevity.\n\n\n\n\n\n\nGiven this information, from how many individuals do we need to collect data to have a reasonable chance of demonstrating this hypothesis is correct? (I.e., What is the sample size?)\n\n\n\n\n\n\n\n\n\n\n\n\nWait, I have no idea how to answer this?\n\n\n\n\nDon’t worry!\nThe goal of this unit is to teach you to tackle this problem.\nLet’s first come up with any approach to compute a sample size, even if we’re not confident in the results.\n\nA few possible places to start:\n\nTake an educated guess: Perhaps you have taken part in or read about similar research before. What order of magnitude seems right for this sort of experiment?\nFind a source: Sample size estimation is a common topic in introductory statistics textbooks. These often include formulas that students can use to compute sample size for specific categories of questions.\nGoogle it: There are many web-based resources (including online calculators) that are designed to enable sample size calculations. Search engines provide a starting point for finding such resource Doing so, you might end up at a website like this or like this.\n\nOr, if you’d like to skip this step, we’ll suggest a sample size of 100.\n\n\n\n\n\n\n\n\nIt you estimated the sample size, what obstacles did you encounter along the way?\n\n\n\n\nSample size calculations aren’t always easy or obvious, even for veteran researchers!\nHere’s a good video of the challenge.\n\n\n\n\n\n\n\n\n\nGiven the description of the scientific hypothesis and experiment, think about what data you would collect and what analyses you would perform to test the hypothesis.\n\n\n\n\nWhat types of values do you expect for each variable? What are their distributions, do you think?\nHow do you expect the variables to be related?\nTry drawing a sketch of what you imagine a successful result might look like?\n(Text) For each participant, we will collect expression levels of substance \\(x\\) and age at death.\n(Text) I expect age at death to increase with \\(x\\).\n(Multiple Choice) Show different plots of \\(x\\) versus age at death, and ask learner to select the plot most consistent with the hypothesis.\n\n\n\n\n\n\n\n\n\nWe provided very little information and asked you to compute the sample size. What other information do you think would be helpful to estimate the sample size?"
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#what-happens-to-the-statistical-power-as-n_resampled-increases",
    "href": "METER_Sample_Size-SHORT-Quarto.html#what-happens-to-the-statistical-power-as-n_resampled-increases",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "What happens to the statistical power as N_resampled increases?",
    "text": "What happens to the statistical power as N_resampled increases?\nTo answer this question, let’s compute the power at different choices of N_resampled and plot the results.\nTo do so, we’ll set K=100 so the code runs in your browser. The plot will be jagged, but you’ll get the idea.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe find that the statistical power increases with N_resampled.\nLarger samples provide more information about the population, leading to more precise estimates of the population parameters. This precision reduces the standard error and widens the gap between the null hypothesis and the alternative hypothesis if there is a true effect, making it easier to detect significant differences. Therefore, increasing the sample size typically increases the power of a statistical test.\n\n\n\n\n\n\nAside: Why do we choose statistical power 0.8?\n\n\n\nChoosing a statistical power of 0.8, or 80%, is a common convention in many fields of research, particularly in the social and biomedical sciences.\nStatistical power is the probability of correctly rejecting a false null hypothesis, thus avoiding a Type II error. A power of 0.8 means there is a 20% chance of a Type II error (failing to detect a true effect). Setting the power at 0.8 provides a reasonable balance between the risks of Type I errors (false positives) and Type II errors (false negatives). Researchers often choose a 5% (alpha=0.05) significance level for Type I errors, aiming to maintain a pragmatic yet cautious approach to declaring findings.\nIncreasing power beyond 0.8 generally requires larger sample sizes, which can escalate the costs and logistical complexity of a study. The choice of 0.8 is considered a good trade-off between increasing precision and controlling operational constraints.\nThe 0.8 level has become somewhat of a standard through historical precedent and its endorsement in statistical texts and guidelines. Researchers often follow these conventions to align with accepted practices, making their studies comparable to others in the field."
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#a-loophole-in-the-statistical-power-increase-alpha",
    "href": "METER_Sample_Size-SHORT-Quarto.html#a-loophole-in-the-statistical-power-increase-alpha",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "A loophole in the statistical power: increase alpha?",
    "text": "A loophole in the statistical power: increase alpha?\nIncreasing the sample size is one way to increase the statistical power.\nHowever, other approaches exist.\nFor example, what if we increase alpha, the Type I error rate?\n\n\n\n\n\n\nHow do we interpret an increase in alpha from 0.05 (the standard value) to 0.1?\n\n\n\n\nIncreasing the significance level alpha from 0.05 to 0.1 means we’re more willing to reject the null hypothesis. We’ll accept a higher risk of rejecting the null hypothesis when it’s actually true (i.e., a higher Type I error).\nAt this looser threshold for significance, it’s easier to declare a result significant. But, the risk of false positives increases - more results will appear significant, but more could be false.\n\n\n\nTo study the impact of increasing alpha on the statistical power, let’s compute it.\nWe’ll do so using a sample size N_resampled=100. (Wait, is that a good choice? Let’s see …)\nWe’ll again set K=100 so the code runs in your browser. The plot will be jagged, but you’ll get the idea.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe find that, at the low sample size N_resampled=100, we can still acheive 80% power if we accept alpha \\(\\approx\\) 0.75.\nDid we find a loophole? Can we get the statistical power we want (0.8) at a low sample size?\nNo, we did not find a loophole.\nWhat we’ve discovered is a fundamental tradeoff in hypothesis testing: you can increase statistical power by increasing alpha, but at a cost.\nStatistical power depends on sample size and the signficance threshold (alpha).\nBy setting alpha=0.75, we’re saying:\n\nI’m willing to reject the null hypothesis 75% of the time even when it is true.\n\nThis results in:\n\nVery high false positive rate (Type I error = 75%)\nArtificially inflated power (since it’s now very easy to reject the null)\nA test that’s statistically meaningless by conventional standards\n\nIt’s like lowering the bar so much that everyone passes the test. Sure, your “pass rate” (power) goes up—but the test no longer distinguishes between those who know the material and those who don’t.\nSo, did we find a loophole?\nNo! We can always increase power by increasing alpha, but that destroys the credibility of our findings. At alpha=0.75 there is a very high probability of being a false positive."
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#resampling-is-a-universal-way-to-calculate-power-and-sample-size-but-only-works-if-the-resampled-data-captures-the-effect-of-interest.",
    "href": "METER_Sample_Size-SHORT-Quarto.html#resampling-is-a-universal-way-to-calculate-power-and-sample-size-but-only-works-if-the-resampled-data-captures-the-effect-of-interest.",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "Resampling is a universal way to calculate power and sample size, but only works if the resampled data captures the effect of interest.",
    "text": "Resampling is a universal way to calculate power and sample size, but only works if the resampled data captures the effect of interest.\nWhen preliminary data exist and these data accurately represent the effect of interest, then resampling is a powerful and universal appraoch to calculate power and sample size.\nIn this case, our preliminary data (see Mini 2) do capture the expected effect of interest - a weak positive relationship between biomarker x and longevity, although when the sample size N is too small, we fail to detect a significant relationship between these features.\nHowever, you might imagine an alternative scenario. What if our preliminary data revealed a negative relationship between biomarker x and longevity? That could happen, for example, if the observations were noisy or the initial sample size N was small. In that case, the resampled data will not capture the effect of interest; resampling these data to calculate power and sample size is a poor choice. Instead, you might pursue alterantive appraoches, including such as Building models without the data.\n\nWarning:\n\nIn this example, we were lucky that the initial draw of a small sample size produced the expected effect. An unlucky sample may have produced (by chance) an opposite effect. In that case, resampling will not produce meaningful power/sample size results. Preliminary data is often important for future experimental design, but it’s important to consider how variability in a small, preliminary dataset can influence power and sample size estimates."
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#turn-to-page-4-summary",
    "href": "METER_Sample_Size-SHORT-Quarto.html#turn-to-page-4-summary",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "Turn to Page 4 Summary",
    "text": "Turn to Page 4 Summary"
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#turn-to-page-4-summary.",
    "href": "METER_Sample_Size-SHORT-Quarto.html#turn-to-page-4-summary.",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "Turn to Page 4 Summary.",
    "text": "Turn to Page 4 Summary."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BU METER",
    "section": "",
    "text": "Sample Size - How much data is enough for your experiment?\n\nInteractive notebook\n\n\n\n\nEvaluate your evaluation methods! A key to meaningful inference.\n\nInteractive notebook\n\n\n\n\nPutting the p-value in context: p&lt;0.05, but what does it REALLY mean?\n\nStatic notebook\n\n\n\n\nReproducible exploratory analysis: Mitigating multiplicity when mining data\n\nStatic notebook"
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html",
    "href": "METER_Inference-SHORT-Quarto.html",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "",
    "text": "Inference is a fundamental concept in both everyday life and scientific investigation. It is the process of drawing conclusions based on evidence and reasoning. Inference allows us to make predictions, understand relationships, and gain insights from data and observations.\n\n\n\n\nDecision-Making: Inference helps us make informed decisions. Whether you’re deciding what to wear based on the weather forecast or determining the best strategy for your experiment, inference plays a crucial role in evaluating options and outcomes.\nUnderstanding Brain Function: Inference helps neuroscientists draw conclusions about brain activity from experimental data, bridging the gap between observed neural signals and underlying brain processes.\nClinical Applications: Inference is essential for diagnosing and treating neurological disorders. By analyzing patient data, clinicians can infer the underlying causes of symptoms and tailor treatments accordingly.\n\n\n\n\n\nWeather Predictions: Meteorologists use data from satellites, weather stations, and historical patterns to infer future weather conditions. This helps us prepare for what’s coming, whether it’s bringing an umbrella or planning for a sunny day.\nMedical Diagnoses: Doctors use symptoms, medical history, and test results to infer the most likely cause of a patient’s condition. This process is critical in providing accurate diagnoses and effective treatments.\nElectrophysiology: Recording electrical activity from neurons allows researchers to infer the roles of specific neurons or networks in processing information.\nBehavioral Studies: By observing behavior in response to stimuli, neuroscientists infer the neural mechanisms underlying perception, decision-making, and learning.\n\n\n\n\n\nHere, we focus on statistical inference - using data from a sample to make inferences about a population. We will learn to apply statistical inference in this Unit.\n\n\n\n\n\nAsk Questions: Cultivate curiosity by asking questions about the world around you. Why did something happen? What might influence this happening? Asking questions leads to deeper understanding and better inference skills.\nGather Evidence: Collect relevant information and data. The more evidence you have, the stronger your inferences will be. Evaluate the quality and reliability of your sources. We can arrive at better conclusions through better data collection.\nThink Critically: Analyze the evidence and consider multiple perspectives. Avoid jumping to conclusions without thorough examination. Critical thinking helps in making sound inferences.\n\n\n\n\n\nIn this Unit, we will practice making inferences from from noisy data.\nTo do so, we will use data from a specific example. We will build models from these data to make inferences about a population (i.e., statistical inference).\nWe will think critically about our inference results, ask questions about the interpretation (i.e., check our model), and update our model to improve the inferences we make from the data."
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#step-1-choose-a-model-for-the-data.",
    "href": "METER_Inference-SHORT-Quarto.html#step-1-choose-a-model-for-the-data.",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Step 1: Choose a model for the data.",
    "text": "Step 1: Choose a model for the data.\nChoosing a model requires we apply our prior knowledge as scientists.\nIn this case, let’s apply our intuition to express our model in words.\n\nInitial Model: I expect more swim lessons reduce the number of drownings.\n\nThat’s a fine model. To perform statistical inference, let’s express our model as an equation:\n\ndrownings = m swim_lessons + b\n\n\n\n\n\n\n\nCompare our model to the equation for a line: ( y = mx + b ). Match each variable to the correct interpretation.\n\n\n\n\n\n\n( y ) →\n\n\n Select… Swim Lessons Drownings Slope Intercept \n\n\n\n\n( x ) →\n\n\n Select… Swim Lessons Drownings Slope Intercept \n\n\n\n\n( m ) →\n\n\n Select… Swim Lessons Drownings Slope Intercept \n\n\n\n\n( b ) →\n\n\n Select… Swim Lessons Drownings Slope Intercept \n\n\n\n Submit\n\n\n\n\n\nHere, our initial model is a line.\n\nThat’s a very simple model of the data.\nWe do not expect a line will capture all of the complexity in our data.\n\nHowever, a line (or linear relationship) often provides a useful first step.\nAnd, a line is relatively simple to compute and interpret."
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#step-2.-use-the-data-to-infer-model-parameters.",
    "href": "METER_Inference-SHORT-Quarto.html#step-2.-use-the-data-to-infer-model-parameters.",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Step 2. Use the data to infer model parameters.",
    "text": "Step 2. Use the data to infer model parameters.\nHaving chosen our model – a line – our next step is to infer the parameters in our model.\nThere are two parameters in the model:\n\nm, the slope, and\nb, the intercept.\n\nWe’re primarily interested in the slope (m). This parameter indicates the relationship between swim_lessons and drownings.\n\n\n\n\n\n\nWhat sign (positive or negative) do you expect for the slope m?\n\n\n\n\n I expect the slope is positive because I expect drownings will increase with swim_lessons.   I expect the slope is negative because I expect drownings will decrease with swim_lessons.  Submit\n\n\n\n\n\n\nWe expect \\(m&lt;0\\).\n\nWe expect a negative relationship between drownings and swim_lessons\nWe expect more swim lessons will reduce the number of drownings.\n\nLet’s infer \\(m\\) from the data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMeaning:\n\nThe slope estimate m represents the estimated change in the number of drownings when the number of swim_lessons increases by 1.\nThe standard_error represents the variability of the slope estimate.\n\nInterpretation:\n\nIf the number of swim_lessons increases by 1, we estimate the number of drowningsto increase by 0.0014, on average.\nThe standard error (0.0002) is small compared to the slope estimate, so the variability in the slope estimate is small; i.e., the estimate is precise.\n\nRemember our model is a line.\nA line is easy to visualize.\nSince we’ve now inferrred the model parameters, let’s visualize the inferred line by plotting it with the data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nLooking at this plot, what is the slope of the inferred line?\n\n\n\n\n The slope is positive – the line tilts up.   The slope is negative – the line tilts down.  Submit"
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#step-3.-summarize-conclusions.",
    "href": "METER_Inference-SHORT-Quarto.html#step-3.-summarize-conclusions.",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Step 3. Summarize conclusions.",
    "text": "Step 3. Summarize conclusions.\nWe have chosen the model (Step 1) and inferred model parameters (Step 2).\nOur last step is to summarize the conclusions of our statistical inference.\nLet’s start by considering an explict numerical conclusion from our model:\n\nHow does the number of drownings change if we increase the number of swim lessons by 1000?\n\n\n\n\n\n\n\nWe find an increase of 0.0014 drownings for a one-unit increase in the number of swim_lessons. How does the number of drownings change when the number of swim_lessons increases by 1000.\n\n\n\nAnswer:\nOur model of the data is a line:\ndrownings = m swim_lessons + b\nWe inferred the model parameter m from the data and found:\ndrownings = 0.0014 swim_lessons + b\nConsider what happens if the number of swim lessons increases by 1. According to our fit model, the number of drownings increases by:\n\\(0.0014 * 1\\) swim lesson \\(= 0.0014\\)\nThat’s a small increase, in both the number of swim lessons and number of drownings.\nAlternately, consider what happens if the number of swim lessons increases by 1000. Plugging into our model, we find the number of drownings increases by:\n\\(0.0014 * 1000\\) swim lessons \\(= 1.4\\)\nThat’s perhaps a more interpertable result; a community making a policy decision to promote more swim lessons (e.g., through subsidies, though advertisements) might expect 1 more drowning.\nIn this way, we can always interpert the results of a model. It’s often useful to plug in values, and see what happens.\n\n\n\nSummary\nWe’ve applied a statistical inference approach and found a compelling result:\n\nThe number of drowings increases with more swim lessons.\n\nThe slope estimate m in our model looks convincing:\n\nThe standard error is small (0.0002) compared to the slope (0.0014); i.e., the estimate is precise.\n\nThese results are certainly passable for peer-reviewed publication and perhaps enough to motivate a new public policy:\n\nTo prevent drownings, discourage swim lessons.\n\n\n\n\n\n\n\nWhat’s our next step?\n\n\n\n\n\nThese results are convincing - let’s advocate for change!\n\n\nI’m not sure yet!"
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#residual-analysis",
    "href": "METER_Inference-SHORT-Quarto.html#residual-analysis",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nMany appraoches exist to check a model (McCullagh & Nelder, 1989; Chapter 12).\nHere, we implement a common and powerful appraoch: residual analysis.\nResiduals are the differences between the observed data and model predicted values.\nIn our case, the residuals are the difference between the estimated drownings from our model (the line we fit to the data) and the actual values for drownings.\nRemember our plot of the estimated model (solid red line) and the data (blue dots):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe residuals are the distance from each blue dot (the data) to the red line (the estimated model).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn the plot above, each orange line indicates a residual.\nIf the model is a good fit to the data, then we expect a patternless set of residuals (i.e., the residuals are randomly scattered around zero).\nAlternatively, if we find patterns in the residuals, we might wonder about our model choice, and consider strategies to update and improve our model.\n\nNOTE: Other approaches exist to check our model. These include goodness-of-fit metrics (like R-squared), comparison with alternative models (using techniques like AIC), and cross-validation to assess how well the model generalizes to new data. There’s unfortunately no one best strategy to check a model. Here we focus on residual analysis because the appraoch is intuitive and provides a good gauge of how our model is doing.\n\nLet’s plot the residuals for our original model:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nDo you see any patterns in this plot of the residuals?\n\n\n\nA: (Short answer)\n\nNo. At first glance, these residuals look mostly patternless.\n\n\n\nOur visualization of the residuals revelas no systematic patterns.\n\nInstead, the residuals appear to fluctuate around 0.\n\n\nThat means our model is ok, right?\nNO!\nThis visualization of the residuals is not very informative.\nThe horizontal axis (the variable index) is arbitrary.\n\nTo observe systematic patterns in the residuals, let’s plot the residuals versus the predictor in our model: the number of swim_lessons.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNow, what pattens do you see in the residuals, when plotted versus the model variable swim_lessons?\n\n\n\nA: (Short Answer)\nInspection of residuals versus swim_lessons reveals two clear patterns:\n\nAs swim_lessons increases, a systematic decrease in the residuals.\nAs swim_lessons increases, the variability of the residuals tends to increase.\n\n\n\nWe conclude from this model check that something isn’t right.\n\nOur model doesn’t adequately describe features in the data.\n\n\n\n\n\n\n\nWe’ve plotted the residuals versus the predictor swim_lessons. We could also plot residuals versus the outcome variable drownings. Doing so, what do you see?\n\n\n\nA: (Extension / Challenge Question)"
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#model-refinement-infinite-choice",
    "href": "METER_Inference-SHORT-Quarto.html#model-refinement-infinite-choice",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Model Refinement: infinite choice",
    "text": "Model Refinement: infinite choice\nOur initial approached modeled swim_lessons versus drownings.\nTo do so, we chose a line as our model (Step 1).\nWe then inferred model parameters from the data (Step 2) and checked the model (Step 3) by visualizing the residuals.\nOur model check suggests the model is inadequate.\n\nWe find systematic discrepancies between the model estimates and data.\n\nTherefore, it’s time to refine our model.\nWe can refine the model in infinite ways.\nFor example, we could consider this model:\n\ndrownings = m swim_lessons + b + a swim_lessons\\(^2\\)\n\nwhere we propose that the number of drownings depends on the number of swim lessons and the number of swim lessons squared.\nOr we might consider:\n\ndrownings = m swim_lessons + b + a swim_lessons\\(^2\\) + c swim_lessons\\(^3\\) + \\(\\log10\\)(swim_lessons)\n\nwhere we propose that the number of drownings depends on the number of swim lessons, the number of swim lessons squared, the number of swim lessons cubed, and log10 of the number of swim lessons.\n\n\n\n\n\n\nWhat other terms might you include in the model?\n\n\n\nA: (Short Answer)\n\nswim_lessons\\(^4\\)\nswim_lessons\\(^{10}\\)\n\n\n\nYour challenge, as a scientist, is to select a meaningful next model from these infinite model choices.\nFor each choice, we can iterate our statistical inference loop (infer the model parameters from data, then check the model).\n\nSelecting a model is an art. You must use your knowledge and intution as a scientist to select model terms.\n\nAs scientists, we use our intuition to rule out ridiculous models.\nFor example, you probably would not consider this model:\n\ndrownings = m swim_lessons + b + a \\(\\sin(\\exp(\\) swim_lessons\\(^{1/3}\\) ))\n\nWhy not consider this model?\n\nBecause the term \\(\\sin(\\exp(\\) swim_lessons\\(^{1/3}\\) )) looks like overly-complicated nonsense.\nWe have intuition that drownings does not depend on swim_lessons in this complicated way.\n\nWhen we observe additional data, we can decide to include that additional information in the model.\nIn this case, we also observe the geographic location (latitude and longitude) of each community in the study.\nLet’s investigate how the residuals relate to the geographic location of the community.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nDo you observe any patterns in the residuals that depend on location?\n\n\n\nA: (Multiple Choice)\n\n(Correct) Yes, the residuals appear to vary with geographic location. Nearer to the ocean, the residuals tend to be more positive (darker blue colors).\n(Incorrect) No, the residuals do not appear to vary with geographic location."
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#conclusions",
    "href": "METER_Inference-SHORT-Quarto.html#conclusions",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Conclusions",
    "text": "Conclusions\nOur analysis of the residuals suggests an important result\n\nOur inital model fit (a line) is no good.\n\nThis is a very useful result.\nOur initial statistical inference produced a counter-intuitive result:\n\nAs swim lessons increase, so do drownings.\n\nBut this initial inference is based on a model - the line - that’s not a good representation of the data.\n\nClear trends exist in the residuals.\n\nChecking the model is our first step to improving the model, and thereby improving the statistical inferences we make from the data.\n\nNOTE: In this case, our model check (Residual Analysis) failed; the resiudals were not randomly scattered around 0.\nBecause the model check failed, we can dismiss the counter-intuitve relationship identified in our initial model (i.e., that more swim_lessons increase the number of drownings) and continue model development.\nHowever, we emphasize that further model development should be pursued regardless of the relationship our initial model identified. Even if the initial statistcal inference proposed an intuitive result (i.e., that more swim_lessons decrease the number of drownings), we would still continue model development; when the model check fails, we’re not satisified with the initial model, no matter the intuitive appeal of the result."
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#step-1-choose-a-model-for-the-data.-1",
    "href": "METER_Inference-SHORT-Quarto.html#step-1-choose-a-model-for-the-data.-1",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Step 1: Choose a model for the data.",
    "text": "Step 1: Choose a model for the data.\nOur refined model of drownings includes two predictors:\n\nThe number of swim lessons (swim_lessons).\nThe distance from the ocean (distance_from_ocean)\n\nmeasured as the geodesic distance in kilometers as the crow flies from Myrtle Beach).\n\n\nOur refined model becomes:\n\nRefined model: drownings = \\(m_1\\) swim_lessons + \\(m_2\\) distance_from_ocean + b\n\nRemember that our initial model - a line in Mini 3 - contained one outcome (drownings) and one predictor (swim_lessons).\nOur refined model contains one outcome (drownings) and two predictors (swim_lessons and distance_from_ocean).\nTherefore, our refined model is a 2-dimensional plane in the 3-dimensional space of the data.\nTo visualize our 3-dimensional data, let’s load these new data and plot it.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nLook at the three-dimensional plot. Can you sketch (or imagine) a plane that slices through these points?\n\n\n\nA: It’s difficult to see, but maybe."
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#step-2.-use-the-data-to-infer-model-parameters.-1",
    "href": "METER_Inference-SHORT-Quarto.html#step-2.-use-the-data-to-infer-model-parameters.-1",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Step 2. Use the data to infer model parameters.",
    "text": "Step 2. Use the data to infer model parameters.\nWith these data and our refined model, let’s continue our statistical inference loop and infer model parameters.\nTo do so, we’ll fit a plane to the data.\nThe idea is exactly the same as fitting a line to the data in our original model.\nBy fitting a plane to the data, we’ll infer three model parameters:\n\n\\(m_1\\), the slope corresponding to the predictor swim_lessons.\n\\(m_2\\), the slope corresponding to the predictor distance_from_ocean.\n\\(b\\), the intercept.\n\nWe’re primarily interested in \\(m_1\\), the slope corresponding to the predictor swim_lessons.\n\nThis parameter indicates the relationship between swim_lessons and drownings.\n\nHowever, we now include the additional predictor distance_to_ocean and also infer its slope (\\(m_2\\)) from the data.\nWe’re not necessarily interested in the relationship between drowings and distance_to_ocean. Motivated by our scientific knowledge (e.g., that drownings may depend on a community’s distance to the ocean) and our residual analysis in the initial model, we include this new predictor.\n\n\n\n\n\n\nWhat sign (positive or negative) do you expect for the slope \\(m_1\\)?\n\n\n\nA: (Multiple Choice)\n\n(Incorrect) I expect the slope is positive because I expect drownings will increases with swim_lessions.\n(Correct) I expect the slope is negative because I expect drownings will decrease with swim_lessions.\n\n\n\n\n\n\n\n\n\nWhat sign (positive or negative) do you expect for the slope \\(m_2\\)?\n\n\n\nA: (Multiple Choice)\n\n(Incorrect) I expect the slope is positive because I expect drownings will increases with distance_from_ocean.\n(Correct) I expect the slope is negative because I expect drownings will decrease with distance_from_ocean.\n\n\n\nWe expect \\(m_1&lt;0\\) and \\(m_2&lt;0\\).\n\nWe expect a negative relationship between drownings and swim_lessons, i.e., we expect increased swim lessons will decrease the number of drownings.\nWe expect a negative relationship between drownings and distance_from_ocean, i.e., we expect increased distance from the ocean will decrease the number of drownings.\n\nLet’s infer \\(m_1\\) and \\(m_2\\) from the data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMeaning:\n\nThe slope estimate m_1 represents the change in the number of drownings for a one-unit change in the number of swim_lessons.\nThe slope estimate m_2 represents the change in the number of drownings for a one-unit change in the distance_from_ocean.\nFor each parameter, the standard_error represents the variability of the slope estimate.\n\nInterpretation of inferred value for m_1:\n\nThe sign of m_1 is negative.\nIf the number of swim_lessons increases by 1, we estimate the number of drownings decreases by 0.0003, on average.\nThe standard error (0.0003) is big compared to the slope estimate, so the variability in the slope estimate is large; i.e., the estimate is unreliable.\n\nInterpretation of inferred value for m_2:\n\nThe sign of m_2 is negative.\nIf the distance_from_ocean increases by 1 kilometer, we estimate the number of drownings decreases by 0.06, on average.\nThe standard error (0.0087) is small compared to the slope estimate, so the variability in the slope estimate is small; i.e., the estimate is precise.\n\nOur refined model is a 2-dimenisonal plane in the 3-dimensional space of variables (swim_lessons, distance_from_ocean, and drownings).\nSince we’ve now inferrred the model parameters from the data, we can visualize the inferred plane.\nIn other words, let’s plot the model with the data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#step-3.-check-and-refine-the-model.",
    "href": "METER_Inference-SHORT-Quarto.html#step-3.-check-and-refine-the-model.",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Step 3. Check and refine the model.",
    "text": "Step 3. Check and refine the model.\nLet’s now check the refined model.\nTo do so, let’s plot the new model errors (i.e., the residuals) versus a predictor in our model (here, the number of swim_lessons).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nWait, I thought this was a 2-dimensional model of 3-dimensional data. Why are the residuals only 1-dimensional?\n\n\n\nA: (Multiple Choice)\n\n(Correct) The residuals are the difference between the observed number of drownings (drowning) and the predicted number of drowning by the model. Both of these quantities are 1-dimensional.\n(Incorrect) The data are 3-dimensional and the model is 2-dimensional so 3-2 = 1.\n\n\n\n\n\n\n\n\n\nWhat pattens do you see in the residuals, when plotted versus the model variable swim_lessons?\n\n\n\nA: (Short Answer)\nInspection of residuals versus swim_lessons reveals one pattern:\n\nAs swim_lessons increases, the variability of the residuals tends to increase.\n\n\n\nRemember that, in our initial model, we observed a systematic trend in the residuals.\n\nOriginal Model: As swim_lessons increase, the average residuals value decreases.\n\nIn our reifned model, visual inspection reveals no systematic trend in the residuals\n\nRefined Model: As swim_lessons increase, the average residuals value remains approximately constant.\n\nThis is a nice improvement compared to our original model.\nIncluding the additional variable distance_to_ocean has reduced a systematic trend in the residuals.\nLet’s also plot the residuals of the new model versus the geographic location.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nDo you observe any patterns in the residuals that depend on location?\n\n\n\nA: (Multiple Choice)\n\n(Incorrect) Yes, the residuals appear to vary with geographic location. Nearer to the ocean, the residuals tend to be more positive (darker blue colors).\n(Correct) No, the residuals do not appear to vary with geographic location."
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#step-4.-summarize-conclusions.",
    "href": "METER_Inference-SHORT-Quarto.html#step-4.-summarize-conclusions.",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Step 4. Summarize conclusions.",
    "text": "Step 4. Summarize conclusions.\nHaving chosen the model (Step 1), inferred model parameters (Step 2), and checked our model (Step 3), our last step is to summarize the conclusions of our statistical inference.\nTo do so, let’s consider explict numerical conclusions from our model:\n\nHow does the number of drownings change if we increase the number of swim lessons by 1000 or distance from the ocean by 100 kilometers?\n\n\n\n\n\n\n\nWe found \\(m_1=-0.0003\\) with standard error \\(0.0003\\). How does the number of drownings change when the number of swim_lessons increases by 1000.\n\n\n\nA:\nOur model of the data is a plane:\ndrownings = \\(m_1\\) swim_lessons + \\(m_2\\) distance_from_ocean + b\nWe inferred the model parameter m_1 from the data and found:\ndrownings = -0.0003 swim_lessons + …\nIf the number of swim lessons increases by 1000, then the number of drownings changes by:\ndrownings = -0.0003 * 1000 = -0.3\nSo, by this calculation, increasing the number of swim lessons by 1000 reduces the number of drownings by -0.3.\nHowever, we have no confidence in this result!\nWe found a standard error of 0.0003 for the estimate of \\(m_1\\). This standard error is as large as the estimate \\(m_1\\) itself.\nWe therefore would not be surprised if repeated experiments found \\(m_1\\) values between approximatley:\n\\(m_1 \\pm 2 * \\mathrm{standard \\, error} = -0.0003 \\pm 2*0.0003 = [-0.0009, 0.0003]\\)\nSo, due to the large standard error, we cannot confidently conclude whether \\(m_1\\) is positive or negative.\nIn other words, we cannot conclude whether increasing swim lessons increases or decreases the number of drownings.\n\n\n\n\n\n\n\n\nWe found \\(m_2=-0.06\\) with standard error \\(0.0087\\). How does the number of drownings change when the distance_to_ocean increases by 100 kilometers.\n\n\n\nA:\nOur model of the data is a plane:\ndrownings = \\(m_1\\) swim_lessons + \\(m_2\\) distance_from_ocean + b\nWe inferred the model parameter m_2 from the data and found:\ndrownings = … -0.06 distance_from_ocean + …\nIf the distance from the ocean increases by 100 kilometers, then the number of drownings changes by:\ndrownings = -0.06 * 100 = -6\nSo, by this calculation, increasing the distance to the ocean by 100 kilometers reduces the number of drownings by -7.\nIn this case, we are confident in this result!\nWe found a standard error of 0.0087 for the estimate of \\(m_2\\). This standard error is much smaller than the estimate of \\(m_2\\) itself.\nWe therefore would not be surprised if repeated experiments found \\(m_2\\) values between approximatley:\n\\(m_2 \\pm 2 * \\mathrm{standard \\, error} = -0.06 \\pm 2*0.0087 = [-0.077, -0.043]\\)\nSo, due to the small standard error, we can confidently conclude that \\(m_2\\) is negative.\nIn other words, we conclude that increasing distance from the ocean decreases the number of drownings."
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#summary-1",
    "href": "METER_Inference-SHORT-Quarto.html#summary-1",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "Summary",
    "text": "Summary\nOur residual analysis suggests an important result: Our refined model (a plane) improves the model fit.\nWe’ve made progress improving our model!\nOur initial model produced a significant - but nonsensical - result:\n\nInitial model conclusion: As swim lessons increase, so do the number of drownings.\n\nHowever, performing our model checks, residual analysis revealed a poor model fit.\nThe poor model fit motivated a cycle through our statistical inference loop.\nWe refined the model by adding another predictor - the distance to the ocean.\nThe refined model changed our results dramatically:\n\nWe improved our model fit (by reducing a systematic trend in the residuals).\n\n\nHowever, we still observe that the variability of the residuals tends to increase with the number of swim lessons.\n\n\nWe found a (weak) relationship between increasing swim lessons and decreasing drownings.\n\n\nWhile this result makes intuitive sense, our evidence for this relationship is not convincing (the variability in the estimate is large).\n\n\nWe found a (strong) relationship between increasing distance from the ocean and decreasing drownings.\n\n\nThis result also makes intuitive sense and our evidence for this relationship is strong (the variability in the estimate is small).\n\nWe show in the next Minis that continued model refinement in our statistical inference loop can further strengthen our results. - If you’re interested, check out the other Minis associated with this Unit.\n\nTo conclude, we applied our statistical inference loop to make inferences from noisy data.\nWe started by choosing a model and inferring the model parameters (i.e., the slopes).\nBut that is not enough!\nWe must also check our model, in our case by plotting the residuals."
  }
]