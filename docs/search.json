[
  {
    "objectID": "METER_Inference-SHORT-Quarto.html",
    "href": "METER_Inference-SHORT-Quarto.html",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "",
    "text": "Inference is a fundamental concept in both everyday life and scientific investigation. It is the process of drawing conclusions based on evidence and reasoning. Inference allows us to make predictions, understand relationships, and gain insights from data and observations.\n\n\n\n\nDecision-Making: Inference helps us make informed decisions. Whether you’re deciding what to wear based on the weather forecast or determining the best strategy for your experiment, inference plays a crucial role in evaluating options and outcomes.\nUnderstanding Brain Function: Inference helps neuroscientists draw conclusions about brain activity from experimental data, bridging the gap between observed neural signals and underlying brain processes.\nClinical Applications: Inference is essential for diagnosing and treating neurological disorders. By analyzing patient data, clinicians can infer the underlying causes of symptoms and tailor treatments accordingly.\n\n\n\n\n\nWeather Predictions: Meteorologists use data from satellites, weather stations, and historical patterns to infer future weather conditions. This helps us prepare for what’s coming, whether it’s bringing an umbrella or planning for a sunny day.\nMedical Diagnoses: Doctors use symptoms, medical history, and test results to infer the most likely cause of a patient’s condition. This process is critical in providing accurate diagnoses and effective treatments.\nElectrophysiology: Recording electrical activity from neurons allows researchers to infer the roles of specific neurons or networks in processing information.\nBehavioral Studies: By observing behavior in response to stimuli, neuroscientists infer the neural mechanisms underlying perception, decision-making, and learning.\n\n\n\n\n\nHere, we focus on statistical inference - using data from a sample to make inferences about a population. Techniques such as hypothesis testing and confidence intervals are key tools in this process. We will learn to apply these tools here.\n\n\n\n\n\nAsk Questions: Cultivate curiosity by asking questions about the world around you. Why did something happen? What might influence this happening? Asking questions leads to deeper understanding and better inference skills.\nGather Evidence: Collect relevant information and data. The more evidence you have, the stronger your inferences will be. Evaluate the quality and reliability of your sources. We can arrive at better conclusions through better data collection.\nThink Critically: Analyze the evidence and consider multiple perspectives. Avoid jumping to conclusions without thorough examination. Critical thinking helps in making sound inferences.\n\n\n\n\n\nIn this Mini, we will practice making inferences from from noisy data.\nTo do so, we will use data from a specific example. We will use these data to make inferences about a population (i.e., statistical inference).\nWe will think critically about our inference results, ask questions about the interpretation, and ultimately improve the inferences we make from the data.\n\n\n\n\nYou receive data from a community organization interested in understanding the impact of swimming lessons on swim safety. The data consist of the following information from in N = 299 communities:\n\nswim_lessons - the number of swim lessons in a community,\ndrownings - the number of drownings (per 100,000 individuals) in a community,\nxy - the geographic location (latitude and longitude) of the community,\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nCode\nswim_lessons, drownings, xy = load_data()\n\n\nWe’re interested in understanding the relationship (if any) between swim_lessons and drownings.\nLet’s start by plotting the data.\n\n\nCode\n# Plot the data.\nplt.figure(figsize=(12, 8))\nplt.scatter(swim_lessons, drownings)\nplt.xlabel('Swim Lessons')\nplt.ylabel('Drownings');\n\n\n\n\n\n\n\n\n\n\nQ: As part of our inference process, let’s ask some questions: What do you observe? How might the number of swim lessons influence the number of drownings?\nA:\n\nThe drowning values are “squished” near 0, with fewer observations at larger values.\nAt first glance, the number of drownings appears to increase with the number of swim lessons … that’s confusing.\n\n\nFor reference (and maybe for future use …) let’s also plot the geographic location of each community.\n\n\nCode\nplot_spatial_coordinates(xy, 0)\n\n\n                                                \n\n\n\nQ: Now, what do you observe? How might the geographic location of each community influence our inference?\nA:\n\nThe communities vary in spatial location, with a high concentration at a point near the ocean.\n\n\n\nNote: Data visualization is often essential to inference. For more information check out Link to Units on Data Visualization. While we utilize data visulzation here, our focus is inference, and specifically statisitcal inference, as we discuss next.\n\n\n\n\n\nNow, let’s make a statistical inference from these data.\nIn general, our goal is to infer from the noisy data a meaningful feature that’s simpler than the data itself.\nThere are many ways to do so.\nHere, we’d like to understand if there’s a relationship between swim_lessons and drownings.\nTo do so, let’s fit a line to the data. This is called a linear regression, which provides a simple way to identify linear relationships between variables. We do not expect linear relationships will capture all of the complexity in our data. However, these relationships often provide a useful first step, and are simple to compute and interpret.\nIn linear regression, the slope of the line provides an estimate of the relationship between the number of drowings and swim_lessons.\n\n\nCode\n# create_dropdown_estimate_regression(swim_lessons, drownings)"
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#the-data-swim-lessons-versus-drownings.",
    "href": "METER_Inference-SHORT-Quarto.html#the-data-swim-lessons-versus-drownings.",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "",
    "text": "You receive data from a community organization interested in understanding the impact of swimming lessons on swim safety. The data consist of the following information from in N = 299 communities:\n\nswim_lessons - the number of swim lessons in a community,\ndrownings - the number of drownings (per 100,000 individuals) in a community,\nxy - the geographic location (latitude and longitude) of the community,\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nCode\nswim_lessons, drownings, xy = load_data()\n\n\nWe’re interested in understanding the relationship (if any) between swim_lessons and drownings.\nLet’s start by plotting the data.\n\n\nCode\n# Plot the data.\nplt.figure(figsize=(12, 8))\nplt.scatter(swim_lessons, drownings)\nplt.xlabel('Swim Lessons')\nplt.ylabel('Drownings');\n\n\n\n\n\n\n\n\n\n\nQ: As part of our inference process, let’s ask some questions: What do you observe? How might the number of swim lessons influence the number of drownings?\nA:\n\nThe drowning values are “squished” near 0, with fewer observations at larger values.\nAt first glance, the number of drownings appears to increase with the number of swim lessons … that’s confusing.\n\n\nFor reference (and maybe for future use …) let’s also plot the geographic location of each community.\n\n\nCode\nplot_spatial_coordinates(xy, 0)\n\n\n                                                \n\n\n\nQ: Now, what do you observe? How might the geographic location of each community influence our inference?\nA:\n\nThe communities vary in spatial location, with a high concentration at a point near the ocean.\n\n\n\nNote: Data visualization is often essential to inference. For more information check out Link to Units on Data Visualization. While we utilize data visulzation here, our focus is inference, and specifically statisitcal inference, as we discuss next."
  },
  {
    "objectID": "METER_Inference-SHORT-Quarto.html#make-an-inference-how-do-the-number-of-swim-lessons-relate-to-drownings",
    "href": "METER_Inference-SHORT-Quarto.html#make-an-inference-how-do-the-number-of-swim-lessons-relate-to-drownings",
    "title": "Evaluate your evaluation methods! A key to meaningful inference.",
    "section": "",
    "text": "Now, let’s make a statistical inference from these data.\nIn general, our goal is to infer from the noisy data a meaningful feature that’s simpler than the data itself.\nThere are many ways to do so.\nHere, we’d like to understand if there’s a relationship between swim_lessons and drownings.\nTo do so, let’s fit a line to the data. This is called a linear regression, which provides a simple way to identify linear relationships between variables. We do not expect linear relationships will capture all of the complexity in our data. However, these relationships often provide a useful first step, and are simple to compute and interpret.\nIn linear regression, the slope of the line provides an estimate of the relationship between the number of drowings and swim_lessons.\n\n\nCode\n# create_dropdown_estimate_regression(swim_lessons, drownings)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BU METER",
    "section": "",
    "text": "Sample Size - How much data is enough for your experiment?\n\nInteractive notebook\n\n\n\n\nEvaluate your evaluation methods! A key to meaningful inference.\n\nStatic notebook\n\n\n\n\nPutting the p-value in context: p&lt;0.05, but what does it REALLY mean?\n\nStatic notebook\n\n\n\n\nReproducible exploratory analysis: Mitigating multiplicity when mining data\n\nStatic notebook"
  },
  {
    "objectID": "OLD/METER_Exploratory-01-07-2025.html",
    "href": "OLD/METER_Exploratory-01-07-2025.html",
    "title": "Reproducible exploratory analysis: Mitigating multiplicity when mining data",
    "section": "",
    "text": "Identifying meaningful patterns and relationships within noisy data is a fundamental component of neuroscience research; however, multiplicity — the practice of conducting multiple simultaneous comparisons - can result in spurious and misleading conclusions. In this unit, learners will understand how multiplicity occurs, its impacts, and strategies to address it.\n# !git clone https://github.com/Mark-Kramer/METER-Units.git\n# import sys\n# sys.path.insert(0,'/content/METER-Units')"
  },
  {
    "objectID": "OLD/METER_Exploratory-01-07-2025.html#rat-brains-position-and-profit-an-introduction-to-exploratory-analysis",
    "href": "OLD/METER_Exploratory-01-07-2025.html#rat-brains-position-and-profit-an-introduction-to-exploratory-analysis",
    "title": "Reproducible exploratory analysis: Mitigating multiplicity when mining data",
    "section": "*1 - Rat brains, position, and profit: an introduction to exploratory analysis",
    "text": "*1 - Rat brains, position, and profit: an introduction to exploratory analysis\n\n# Load modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom tqdm import tqdm\nfrom matplotlib.colors import ListedColormap\n# Load custom functions\nfrom exploratory_functions import *"
  },
  {
    "objectID": "OLD/METER_Exploratory-01-07-2025.html#extra-extra-rat-brains-crack-the-stock-market-code",
    "href": "OLD/METER_Exploratory-01-07-2025.html#extra-extra-rat-brains-crack-the-stock-market-code",
    "title": "Reproducible exploratory analysis: Mitigating multiplicity when mining data",
    "section": "EXTRA! EXTRA! RAT BRAINS CRACK THE STOCK MARKET CODE!",
    "text": "EXTRA! EXTRA! RAT BRAINS CRACK THE STOCK MARKET CODE!\nIn a dazzling demonstration of rodent ingenuity—or perhaps sheer luck—researchers have harnessed the firing neurons of rat motor cortices to predict movements in the U.S. stock market. That’s right, folks! A team from Michigan and Georgia linked rat brain activity to Wall Street ticker tapes, proving that whiskers may rival Wall Street wits. By monitoring the firing rates of 94 neurons across three rats, these scientists uncovered correlations between neural activity and the daily closing prices of stocks on NASDAQ, the NYSE, and the American Stock Exchange. The Coca-Cola stock price, for instance, danced in sync with the neurons like Fred and Ginger at the Ziegfeld Follies.\nBut wait, there’s more! The researchers didn’t stop at finding patterns — they dove headfirst into the trading floor with a predictive model based on neural firing rates. Rats’ neural spikes gave the orders: buy, sell, or hold. And the results? An impressive 43% increase in a simulated portfolio value, turning an initial $$$1,000 investment into a snappy $$$1,435 over just 20 trading days. Forget the contrarian strategies of hedge fund honchos—our furry friends seem to have cracked the code.\nAnd here’s the kicker, folks: this isn’t just about making a buck. The findings suggest a mysterious connection between rat neural activity and human economic behavior. The researchers propose a grand theory linking the creatures of Earth to the ebbs and flows of societal urges, tying their work to theories like Gaia’s interconnected organism hypothesis. So, the next time someone says, “it’s a rat race out there,” remember—they might just be running the show!"
  },
  {
    "objectID": "OLD/METER_Exploratory-01-07-2025.html#thats-quite-a-story",
    "href": "OLD/METER_Exploratory-01-07-2025.html#thats-quite-a-story",
    "title": "Reproducible exploratory analysis: Mitigating multiplicity when mining data",
    "section": "That’s quite a story …",
    "text": "That’s quite a story …\nUsing noisy data recorded from rodent brains, the authors found a relationship between neural activity and stock prices. At first glance, these results may seem consistent with well-established observations that neural activity encodes a rodent’s location in its world citation. However, you might also be (reasonably) skeptical. How could neurla\n\nIntroduction:\n\nBackground studies suggest relationships between neural activity and phenomena in the world.\nFor example, action potentials (or “spikes”) occurring in a rodent’s brain indicate:\n\nthe rodent’s location in the world (citation, Nobel Prize)\nstock prices of companies on the NYSE (citation, Annals of Improbable Research).\n\nIn this unit, you’ll explore a large data set to search for these types of relationships.\nYou’ll be confronted with a challenge: how will you decide among many possible relationships which are meaningful?\n\n\n\nQ: When exploring data, the number of relationships between the things you observe increases quickly with the number of observed items. To get a sense for this, consider the results in (citation, Annals of Improbable Research). How many relationships were tested?\nA: Activity from 94 neurons was compared with activity from 4195 stocks. The total number of comparisons considered is 94*4195 = 381,745.\nInterpretation: That number can get very big with modern big data observations! How do you decide which of these comparisons are meaningful?\n\n\nQ: Consider N neurons and M signals. How many possible combinations of relationships exist between the neurons and signals?\nA: N*M\nInterpretation: That number can get very big with modern big data observations! How do you decide which of these comparisons are meaningful?"
  },
  {
    "objectID": "OLD/METER_Exploratory-01-07-2025.html#visualization-the-first-step-in-exploration",
    "href": "OLD/METER_Exploratory-01-07-2025.html#visualization-the-first-step-in-exploration",
    "title": "Reproducible exploratory analysis: Mitigating multiplicity when mining data",
    "section": "[2 - Visualization, the first step in exploration]",
    "text": "[2 - Visualization, the first step in exploration]\nMotivated by previous studies, you receive data from a collaborator interested in understanding the relationship between neural spiking in the rodent brain, rodent position, and stock prices on the NYSE. The data consist of the following information:\n\nspikes - the action potentials (or “spikes”) generated by 200 neurons,\nsignals - the rodent position (one of the signals) and the price of 99 stocks.\n\nBoth the spikes and signals are recorded simultaneously, every 10 ms for 0.5 s, resulting in a total of 500 time points.\nWrinkle: For the signals, our collaborator isn’t sure which of the 100 signals correpsonds to the rodent position. This happens sometimes. We that 1 of the 100 signals is the rodent position, but not which one. That’s ok for now, in our exploratory analysis.\n\nspikes, signals, t = load_data()\n\nWe’re interested in understanding the relationship (if any) between spikes and signals.\nLet’s start by investigating the structure of the data.\n\nprint(spikes.shape)\nprint(signals.shape)\n\nBoth spikes and signals consist of 500 time points (the number of rows). We collect data from 200 neurons and 100 signals (the number of columns).\nYou might think of these variables as rectangles (or matrices), where each row indicates a time point, and each column indicates a neuron or signal:\n\n\n\ntitle\n\n\nLet’s plot the data from one neuron in spikes:\n\n# Plot the spiking from an example neuron.\nplt.figure(figsize=(12, 2))\nplot_spike_train(t, spikes[:,0])  # Plot the first spike train.\n\n\nQ: How do these data represent “spikes”?\nA: In this signal, the value 1 represents a spike occurs at time ‘t’. The value 0 indicates that no spike occurs at time ‘t’. In the figure, we use a black circle to indicate the times of spikes.\n\nLet’s also plot the data from one signal in signals:\n\n# Plot the spiking from an example neuron.\nplt.figure(figsize=(12, 2))\nplt.plot(t, signals[:,0]);\n\n\nQ: What features do you notice in the signal?\nA: It’s “wiggly” and maybe there are rhythmic oscillations, but it’s hard to tell. It’s “noisy”.\n\n\nQ: Compare the plots of example spiking and an example signal. Are they related?\nA: It’s very difficult to tell what relationship - if any - exists between the signals.\nInterpretation: Visual inspection is not especially useful in this example.\n\nTo more directly compare the spikes and signals, let’s plot one atop the other:\n\nplt.figure(figsize=(12, 2))\nplt.plot(t, signals[:,0]);        # Plot the first signal.\nplot_spike_train(t, spikes[:,0])  # Plot the first spike train.\n\n\nQ: Compare the spikes (black dots) and signal (blue trace) in the plot above. Are they related?\nA: It’s very difficult to tell what relationship - if any - exists between the signals.\nInterpretation: Visual inspection is not especially useful in this example.\n\n\nQ: We’ve used visual inspection to compare the spike train from one neuron (the first column of spikes) and one signal (the first column of signals). We didn’t see an obvious relationship. However, there are many more relationships to compare. Repeat this analysis to compare each spike train (columns 1 to 200 of spikes) with each signal (columns 1 to 100 of signals). Do you observe any relationships.\nA: Wait! That’s 100*200 = 20,000 plots to visualize …\n\n\nAlert: Wait, we can’t use visualization alone!\n\n\nThere are 200 neurons and 100 signals, resulting in 20,000 pairs to consider.\nWe can’t possibly visualize all of those.\nInstead, we need to perform statistical tests."
  },
  {
    "objectID": "OLD/METER_Exploratory-01-07-2025.html#profit-rat-brains-for-high-frequency-trading-in-the-nyse",
    "href": "OLD/METER_Exploratory-01-07-2025.html#profit-rat-brains-for-high-frequency-trading-in-the-nyse",
    "title": "Reproducible exploratory analysis: Mitigating multiplicity when mining data",
    "section": "*3- Profit! Rat brains for high-frequency trading in the NYSE",
    "text": "*3- Profit! Rat brains for high-frequency trading in the NYSE\nLet’s now investigate the pairwise associations between the spiking of 200 neurons in the rat brain, the rat position, and 99 stock prices.\n\nQ: We have 200 neurons and 100 signals. How many possible combinations of relationships exist between the neurons and signals?\nA: 200*100 = 20,000\nInterpretation: That’s a large number! I wonder if we’ll find something interesting in there …\n\nTo compare the spikes and signals, we perform a statisitcal test of the association between each pair of data. The details of the test are not important. What is important is that each test produces a p-value, and small p-values indicate statistically significant associations (link to Unit:“Putting the p-value in Context”).\nLet’s compute those p-values now. This step is slow, because there are many p-values to compute!\n\np = compute_p_values(spikes, signals)\n\nFor each neuron-signal pair, we estimate the association with an associated p-value. Let’s visualize those p-values.\n\nplt.figure(figsize=(8, 8))\nplt.imshow(p); plt.colorbar();\nplt.xlabel('Neuron number'); plt.ylabel('Signal number');\n\n\nQ: What do you observe? Can you find the meaningful relationships between neurons and signals, if any?\nA: We’re looking for small values (the darkest blue) in the image. It’s very difficult to see the meaningful relationships. The image is noisy.\n\nLet’s look for the “most significant” association - that with the smallest p-value - and plot the spikes and signal for this association.\n\nmin_index_flat = np.argmin(p)\nmin_index = np.unravel_index(min_index_flat, p.shape)\nprint(min_index)\nplt.figure(figsize=(12, 2))               # For the association with the smallest p-value,\nplt.plot(t,        signals[:,min_index[0]]);        # ... plot the signal,\nplot_spike_train(t, spikes[:,min_index[1]])  # ... plot the spike train.\nplt.xlim([0, 100])\n\nTo isolate meaningful relationships, let’s find all associations in which p&lt;0.05, the standard threshold for signficance applied in practice (link to Unit:“Putting the p-value in Context”).\n\nnp.sum(p &lt; 0.05)\n\n\nQ: Interpert this result. What do you conclude about the data?\nA: Of the 20,000 possible associations between spikes and signals, we detect 1043 associations with p&lt;0.05.\nInterpretation: Rat brains are associated with stock prices! Let’s develop a new strategy for profitable high-frequency stock trading using rat brain neuron spiking.\n\n\nReflection:\n\nHmm, there are a lot of significant relationships - this is so easy! Why isn’t this used more often? Something must not be exactly correct…\n\n\n\nAlert: Wait, this doesn’t make sense!\n\n\nHow can the spike timing in a rat brain relate to stock market prices?\nWe’ve identified all associations with p&lt;0.05 … Is that the right choice?\n\n\n\nMoment of tension:\n\nHook the learner - “something isn’t right and I want to know why.”"
  },
  {
    "objectID": "OLD/METER_Exploratory-01-07-2025.html#not-so-fast-finding-meaningful-relationships-after-youve-tested-everything.",
    "href": "OLD/METER_Exploratory-01-07-2025.html#not-so-fast-finding-meaningful-relationships-after-youve-tested-everything.",
    "title": "Reproducible exploratory analysis: Mitigating multiplicity when mining data",
    "section": "*4- Not so fast … finding meaningful relationships after you’ve tested everything.",
    "text": "*4- Not so fast … finding meaningful relationships after you’ve tested everything.\nIn Mini 3, we assessed associations between 200 neurons and 100 signals.\nThis exploration led to many (20,000) statistical tests.\nWe now need to correct for multiplicity.\nWhen conducting multiple hypothesis tests, increased error rates occur because each test has a chance of incorrectly rejecting the null hypothesis (a false positive). This error, typically called the Type I error, is the probability of a single test falsely claiming a statistically significant effect.\nAs more tests are performed, the cumulative probability of committing at least one Type I error across all these tests increases, leading to an overall higher error rate for the set of tests than for any individual test. This phenomenon is often referred to as the “multiple comparisons problem” or “multiplicity.”\nTo mitigate multiplicity when mining data, we must account for increased error rates due to conducting multiple hypothesis tests on the same dataset.\n\nQ: In our initial analysis, we chose a Type I error rate (\\(\\alpha\\)) of 0.05. So, each test has a probability of 0.05 of falsely claiming a statistically significant effect.\nWe performed 20,000 total tests. How many false statistically significant effects do we expect?\nA: 20,000 * 0.05 = 1000\nInterpretation: Because we conducted so many tests (20,000), and selected all associations with p&lt;0.05, we expect 1000 false positives. In Unit 3, we found 1043 associations with p&lt;0.05. We conclude that most of these associatiosn are false positives!\n\nTo correct for multiplicity, let’s apply a popular procedure: the Bonferroni correction.\nThe Bonferroni correction reduces the Type I error rate by dividing the desired overall significance level (\\(\\alpha\\)) by the number of tests performed.\n\nQ: Compute the Bonferroni correction using our desired overall significance level (\\(\\alpha=0.05\\)) and the number of tests performed.\nA: 0.05 / 20000 = 0.0000025\nInterpretation: Using the Bonferroni correction, we now declare associations as significant if p &lt; 0.0000025.\n\nLet’s apply the Bonferroni correction to our matrix of p-values (p) and determine the number of significant associations, after correcting for multiplicity.\n\nnp.sum(p &lt; 0.05/20000)\n\n\nQ: Interpert this result. What do you conclude about the data?\nA: Of the 20,000 possible associations between spikes and signals, we detect 0 associations with p&lt;0.05 after applying the Bonferroni correction.\nInterpretation: Rat brains not associated with stock prices or positions.\n\n\nAlert: Wait, this doesn’t make sense!\n\n\nMuch work (including Noble Prize work) has established a relationship between rat neuron spiking and rodent position.\nWe’ve identified all associations with p&lt;0.05 after Bonferroni correction … Is that the right choice?\n\n\n\nExtension / simulation (DRAFT):\n\nFlip 5 coins. Are you likely to get all heads?\n\nNo. That’s unlikely when you do it once.\n\nNow, ask 1000 people to repeat this 5 coin flip process. Do you think someone will get all heads?\n\nYes. By chance, someone might flip 5 coins and get all heads.\nThe goal of accounting for multiplcity is to control for these by chance occurrences."
  },
  {
    "objectID": "OLD/METER_Exploratory-01-07-2025.html#rat-brains-either-predict-everything-or-nothing",
    "href": "OLD/METER_Exploratory-01-07-2025.html#rat-brains-either-predict-everything-or-nothing",
    "title": "Reproducible exploratory analysis: Mitigating multiplicity when mining data",
    "section": "*5- Rat brains either predict everything or nothing?",
    "text": "*5- Rat brains either predict everything or nothing?\nWe’ve applied two strategies to confront multiplicity in exploratory analysis\n\nDo nothing (Mini 3)\nBonferroni correction (Mini 4)\n\nThe two strategies give completely different results. Either neurons in the rat brain spike with many associations to observed signals (Mini 3) or no associations (Mini 4). And, neither result makes sense when compared to the existing literature. So, now what?\nMany strategies exist to correct for multiplicity. There’s no single “right” approach.\n\nOur first approach (do nothing) is too lenient - we allow two many false positives. And, in doing so, we draw a ridiculous scientific conclusion: action potentials in the rat brain predict prices on the stock market.\nOur second approach (Bonferroni correction) is too strict - we allow too few false positives. And, in doing so, we find no evidence for a well-established scientific conclusion: action potentials in the rat brain encode position.\n\nPerhaps we can find an intermediate approach, that allows neither too many nor too few false positives …\nIn what follows, you can explore alternatives to our choices above, and see how these choices impact your results.\nChoose your own adventure!\n\nFalse Discovery Rate or FDR (Mini 6)\nSplit the data (Mini 7)"
  },
  {
    "objectID": "OLD/METER_Exploratory-01-07-2025.html#fdr-a-less-conservative-approach-to-multiplicity.",
    "href": "OLD/METER_Exploratory-01-07-2025.html#fdr-a-less-conservative-approach-to-multiplicity.",
    "title": "Reproducible exploratory analysis: Mitigating multiplicity when mining data",
    "section": "6- FDR: a less conservative approach to multiplicity.",
    "text": "6- FDR: a less conservative approach to multiplicity.\nYou’ve chosen one approach to mitigate the impact of performing multiple statistical tests: calculate the False Discovery Rate (FDR).\nTo do so, we’ll use the Benjamini-Hochberg procedure, implemented in these steps:\n\nRank your p-values from the smallest to the largest. Call these ranked p-values \\(p_R\\).\nAssign each p-value a rank, \\(R\\), where the smallest p-value has rank 1, the next smallest has rank 2, and so on.\nCalculate the critical value for each p-value using the formula:\n\n\\(CriticalValue =(R/N) \\alpha\\)\nwhere \\(N\\) is the total number of tests, and \\(\\alpha\\) is the desired overall significance level (typically 0.05 for 5% FDR).\n\nCompare each p-value to its corresponding critical value. The largest rank, \\(R\\), for which \\(p_R \\leq Critical Value\\) is considered significant, and all p-values with ranks less than or equal to this \\(R\\) are considered significant.\n\nTo build some intuition for these steps, let’s start with a simple example.\nConsider an experiment in which you perform 5 hypothesis tests and collect 5 p-values:\np-value: [0.1, 0.12, 0.001, 0.03, 0.045]\nWe conducted 5 tests, and decide to correct for multiplicity.\nLet’s use the p-values to perform each step of the Benjamini-Hochberg procedure.\n\nQ: Perform Step 1. Rank the p-values.\nA: [0.001, 0.03, 0.045, 0.1, 0.12]\n\n\nQ: Perform Step 2. Assign each p-value a rank.\nA: p-value, Rank\n0.001, 1\n0.03, 2\n0.045, 3\n0.1, 4\n0.12, 5\n\n\nQ: Perform Step 3. Calculate the critical value.\nA: The critical value for each p-value is (Rank / Total number of tests) * Alpha level. Let’s use an alpha level of 0.05, the critical values for each rank would be:\nRank, Critical values\n1, (1/5)*0.05 = 0.01\n2, (2/5)*0.05 = 0.02\n3, (3/5)*0.05 = 0.03\n4, (4/5)*0.05 = 0.04\n5, (5/5)*0.05 = 0.05\n\n\nQ: Perform Step 4. Compare each p-value to its corresponding critical value.\nA: Let’s do this for each p-value, starting from the smallest p-value:\np-value &lt; or &gt; Critical values\n0.001 vs (1/5)*0.05 = 0.01 … &lt;, significant\n0.03 vs (2/5)*0.05 = 0.02 … &lt;, significant\n0.045 vs (3/5)*0.05 = 0.03 … &gt;, not significant\n0.1 vs (4/5)*0.05 = 0.04 … &gt;, not significant\n0.12 vs (5/5)*0.05 = 0.05 … &gt;, not significant\nThus, p-values of 0.001 and 0.03 are considered significant under an FDR threshold of 5%. The p-values of 0.045, 0.1, 0.12 do not meet the significance threshold in this analysis.\n\n\nQ: In this illustrative example, we found two p-values are considered significant using the FDR procedure. What would you find if you instead used a Bonferroni correction?\nA: For the Bonferroni correction, we use the threshold 0.05/5 = 0.01 for each comparison:\np-value &lt; or &gt; Critical values\n0.001 vs (1/5)*0.05 = 0.01 … &lt;, significant\n0.03 vs (1/5)*0.05 = 0.01 … &gt;, not significant\n0.045 vs (1/5)*0.05 = 0.01 … &gt;, not significant\n0.1 vs (1/5)*0.05 = 0.01 … &gt;, not significant\n0.12 vs (1/5)*0.05 = 0.01 … &gt;, not significant\nWe now find that only one p-values (0.001) is considered significant under a Bonferroni correction.\n\nNow, having built some intuition for computing the FDR using the Benjamini-Hochberg procedure, let’s apply it to our data of interest.\n\np_values_signficant_after_FDR = fdr(p)\n\nThe function fdr returns whether each p-value in p remains signficant after correcting for multiplicity using FDR.\nLet’s display the p-values that remain significant after correcting for multiplicity using FDR:\n\nplt.figure(figsize=(8, 8))\ncmap = ListedColormap(['white', 'red'])\nplt.imshow(p_values_signficant_after_FDR, cmap=cmap)\nplt.xlabel('Neuron number'); plt.ylabel('Signal number');\n\n\nQ: Examine the figure above. Do you see any red dots, indicating significant relationships between neurons and signals after correcting for multiplicity using FDR?\nA: There are two red dots, between:\n“Signal 0” and “Neuron 0” – i.e., the first signal and first neuron.\n“Signal 0” and “Neuron 24” – i.e., the first signal and 25th neuron.\nInterpretation: After correcting for multiplicity, two neurons in the rat brain are associated with the same signal (“Signal 0”). We return to our collaborators, and they confirm that “Signal 0” corresponds to the rodent’s position.\n\n\nRelief\n\nWe’ve corrected for multiple comparisons and find a result that makes sense. These results are consistent with the existing theory that neuron’s in the brain encode rodent position."
  },
  {
    "objectID": "OLD/METER_Exploratory-01-07-2025.html#split-your-data-to-test-your-conclusions.",
    "href": "OLD/METER_Exploratory-01-07-2025.html#split-your-data-to-test-your-conclusions.",
    "title": "Reproducible exploratory analysis: Mitigating multiplicity when mining data",
    "section": "7- Split your data to test your conclusions.",
    "text": "7- Split your data to test your conclusions.\n(Pending)\n(Use first half of data, in time)\n\nT = np.shape(spikes)[0];\np_split = compute_p_values(spikes[:,0:int(T/2)-1], signals[:,0:int(T/2)-1])\n\n(Find candidate significant associations)\n\ncandidate_significant_pairs = (p_split&lt;0.05)\nprint(sum(candidate_significant_pairs.flatten()))\n[i_signals,i_spikes] = np.where(candidate_significant_pairs)\n\n(Test these predicted associations in the second half of data)\n\np_test = np.zeros(np.shape(i_signals))\nfor index, element in enumerate(i_signals):\n    X = signals[int(T/2):, i_signals[index]]       # Predictor variable\n    y = spikes[int(T/2):, i_spikes[index]]        # Response variable\n    X = sm.add_constant(X)  # Adding a constant column for the intercept\n    glm_model = sm.GLM(y, X, family=sm.families.Poisson())\n    glm_results = glm_model.fit()\n    p_test[index] = glm_results.pvalues[1]\n\n(Identify significant associations)\n\ni0 = np.where(p_test&lt;0.05/np.size(i_signals))\nprint('Signal', i_signals[i0], ', Spike', i_spikes[i0])"
  },
  {
    "objectID": "OLD/METER_P_Values-01-08-2025.html",
    "href": "OLD/METER_P_Values-01-08-2025.html",
    "title": "Putting the p-value in Context",
    "section": "",
    "text": "Neuroscience researchers typically report p-values to express the strength of statistical evidence; but p-values are not sufficient on their own to understand the meaning and value of a scientific inference. In this unit, learners will learn how to interpret the p-value, how to express the size of an effect and uncertainty about a result, and how to interpret results at both the individual and population levels.\n# !git clone https://github.com/Mark-Kramer/METER-Units.git\n# import sys\n# sys.path.insert(0,'/content/METER-Units')"
  },
  {
    "objectID": "OLD/METER_P_Values-01-08-2025.html#but-are-we-sure",
    "href": "OLD/METER_P_Values-01-08-2025.html#but-are-we-sure",
    "title": "Putting the p-value in Context",
    "section": "But are we sure?",
    "text": "But are we sure?\n\nQ: Review the characteristics of the during and post-treatment data (Mini 1). How might these characteristics impact the p-values?\nA: This is a very important question … and we’re not going to answer it yet.\nBut here’s a hint: notice that we collect many more samples post-treatment, compared to during treatment. I wonder how this might impact our p-value comparisons …\n\n\nAlert: Wait, I’m not so sure … \n\n\nWhy is the word seem in bold?\nWhy did you ask me to review the characteristics of the data, and think about how this might impact the data?\n\n\n\nMoment of tension:\n\nHook the learner - “something isn’t right and I want to know why.”\n\n\n\nQ: EXTENSION EXERCISE: We’ve examined the spindle activity during treatment and post-treatment. How would you test the null hypothesis of spindle activity = 0 during the baseline condition?\n\n\nQ: Before finishing this Mini, let’s review:\n\nWhat is the scientific question we originally sought to answer (Mini 2)? (Multiple Choice)\nWhat is the scientific question we answered in this Mini? (Multiple Choice)\nWhat statistical approach did we use to answer this scientific question? (Multiple Choice)\n\nA: 1. Does the spindle activity during treatment differ from the baseline spindle activity?\n\nDoes the spindle activity post-treatment differ from the baseline spindle activity?\nTo answer the scientific question in this Mini, we assumed a null hypothesis of no difference in spindle activity post-treatment compared to 0. We then tested this null hypothsis for each subject, and computed a p-value. We rejected the null hypothesis if the p-value was small enough. Notice that, in doing so, we computed a p-value to provide a yes/no answer to the question: do we have evidence that the spindle activity during post-treatment differs from 0?"
  },
  {
    "objectID": "OLD/METER_P_Values-01-08-2025.html#not-so-fast-visualize-always.",
    "href": "OLD/METER_P_Values-01-08-2025.html#not-so-fast-visualize-always.",
    "title": "Putting the p-value in Context",
    "section": "4- Not so fast: visualize, always.",
    "text": "4- Not so fast: visualize, always.\nWe may have found an interesting result: a lower spindle activity post-treatment, but no effect during treatment.\nTo do so, we analyzed and compared p-values, testing specific null hypotheses.\nWe’ve hinted above that something isn’t right … let’s now dive in and identify where we went wrong.\n\nQ: Let’s remember - what was the original scientific question we wanted to test?\nA: Does the spindle activity during treatment differ from the baseline spindle activity?\nQ: How did we test this original scientific question?\nA: We computed p-values to test the null hypothesis of no difference in spindle rate during treatment from 0.\n\nOur initial hypothesis “failed” … the p-values were too big.\nSo, we investigated the spindle activity post-treatment.\nWe applied the same strategy of computing p-values to test the null hypothesis that the spindle activity is 0.\nWe have focused (almost exclusively) on p-values.\nLet’s again plot the p-values during and post-treatment:\n\nplt.figure(figsize=(12, 2))\nl1 = plt.stem(p_value_during, 'or')\nl2 = plt.stem(p_value_post, 'ob')\nplt.yscale('log')\nplt.axhline(y=0.05/20, color='r', linestyle='--')\nplt.xticks(np.arange(0,20));\nplt.xlabel('Subject'); plt.ylabel('p value');\nplt.title('During Treatment (red), Post-treatment (blue)');\n\n\nQ: For each subject, compare the p-values during treatment (red) versus post-treatment (blue). What do you observe?\nA: We oberve a post-treatment effect in many subjects (the p-value is small), but not during treatment (the p-value is big).\n\nSo far, we’ve used the spindle measurements to compute p-values.\nBut, we’ve almost completely ignored the spindle measurements themselves!\nNow, let’s return to the spindle measurements and look at those values directly.\n\nQ: Given our analysis of the p-values, how do you expect the spindle measurements behave during treatment and post-treatment?\nA: - Because we do not find significnat p-values during treatment, I expect these values to appear near 0. - Because we do find significnt p-values post-treatment, I expect these values to differ from 0.\n\n\nplt.figure(figsize=(12, 2))\nfor k in np.arange(0,20):\n    plt.plot(np.tile(k,(840,1))-0.25, baseline[:,k], '.', color='k')\nfor k in np.arange(0,20):\n    plt.plot(np.tile(k,(30,1)), during_treatment[:,k], '.', color='r')\nfor k in np.arange(0,20):\n    plt.plot(np.tile(k,(840,1))+0.25, post_treatment[:,k], '.', color='b')\nplt.axhline(y=0, color='k')\nplt.xlabel('Subject'); plt.ylabel('Spindle activity'); plt.title('Baseline (black), During Treatment (red), Post-Treatment (blue)'); #plt.ylim([0,1]);\nplt.xticks(np.arange(0,20));\n\n\nQ: Looking at the plots of spindle measurements, what do you observe? More specifically: 1. Do you observe an effect during treatment? 2. Do you obesrve an effect post-treatment? 3. Are these plots consistent with you p-value results?\nA: 1. Yes, sort of … the spindle measurements during treatment appear larger than post-treatment. How can that be if the p-values are big? 2. Not really … because we find significant p-values post-treatment, I expect these values to differ from 0 and from the distribution of baseline values. But I don’t see that here … 3. Not really … we concluded that there’s an effect post-treamtnet, but not during treatment. However, these plots of spindle measurements aren’t consistent with our conclusions.\n\nIt’s nice to visualize all of the data, but doing so can also be overwhelming.\nLet’s summarize the spindle measurements in for each subject by ploting the mean and the standard error of the mean.\n\n# Plot the spiking from an example neuron.\nplt.figure(figsize=(12, 2))\nfor k in np.arange(0,20):\n    mn = np.mean(baseline[:,k]);\n    sd = np.std( baseline[:,k]);\n    K  = np.shape(baseline)[0];\n    plt.plot(k-0.2, mn, 'o', color='k')\n    plt.plot([k-0.2,k-0.2], [mn-2*sd/np.sqrt(K), mn+2*sd/np.sqrt(K)], color='k')\nfor k in np.arange(0,20):\n    mn = np.mean(during_treatment[:,k]);\n    sd = np.std( during_treatment[:,k]);\n    K  = np.shape(during_treatment)[0];\n    plt.plot(k, mn, 'o', color='r')\n    plt.plot([k,k], [mn-2*sd/np.sqrt(K), mn+2*sd/np.sqrt(K)], color='r')\nfor k in np.arange(0,20):\n    mn = np.mean(post_treatment[:,k]);\n    sd = np.std( post_treatment[:,k]);\n    K  = np.shape(post_treatment)[0];\n    plt.plot(k+0.2, mn, 'o', color='b')\n    plt.plot([k+0.2,k+0.2], [mn-2*sd/np.sqrt(K), mn+2*sd/np.sqrt(K)], color='b')\nplt.axhline(y=0, color='k')\nplt.xlabel('Subject'); plt.ylabel('Spindle activity'); plt.title('Baseline (black), During Treatment (red), Post-Treatment (blue)');\nplt.xticks(np.arange(0,20));\n\n\nQ: Looking at the summary plots of spindle measurement means and standard error of the means for each subject, what do you observe? More specifically: 1. Do you observe an effect during treatment? 2. Do you obesrve an effect post-treatment? 3. Are these plots consistent with you p-value results?\nA: 1. Yes … the spindle measurements during treatment appear larger than post-treatment. How can that be if the p-values are big during treatement? 2. Not really … because we find significnt p-values post-treatment, I expect these values to differ from 0. But I don’t see that here, the black and blue dots appear to overlap near 0 … 3. Not really … we concluded that there’s an effect post-treatment, but not during treatment. However, these plots of spindle measurements aren’t consistent with our conclusions.\n\nLet’s summarize what we’ve found so far:\n\n\n\n\n\n\n\n\nState\np-values\nspindle activity\n\n\n\n\nDuring treatment\np&gt;0.05/20 (not significant)\nmean spindle activity &gt; 0\n\n\nPost-treatment\np&lt;&lt;0.05/20 (signficiant)\nmean spindle activity \\(\\approx\\) 0.\n\n\n\nSomething’s not adding up here …\nLooking at the spindle activity plots, we observe:\n\nthe mean spindle activity during treatment often exceeds the mean spindle activity at baseline.\n\nThat’s the oppostive conclusion we made from the p-values!\nHmm … we better think carefully about the paper title …\n\nAlert: We’re made multiple statistial errors:\n\nCompared p-values\nExploratory analysis\n\n\n\n\nQ: Why do the spindle activities during treatment exceed the baseline spindle activity, but p&gt;0.05? And, why are the post-treatment spindle activities so near the baseline spindle activity, but p&lt;&lt;0.05?\nA: The p-value measures the strength of evidence against the null hypothesis.\n\nTwo things impact the strength of evidence: - the effect size (i.e., the values of the spindle activity). - the number of observations\n\nQ: How do the number of observations differ during treatment versus post-treatment? How might this impact the results?\nA: - We have many more observations post-treatment. Therefore, we can accumulate enough evidence to detect a weak effect post-treatment. - We have few observations during treatment. Therefore, even though the effect is strong, we don’t have enough evidence to declare an effect during treatment.\n\n\nConclusion / Summary / Morale:\nWe began with the scientific statement:\n“I expect that during treatment the spindle activity exceeds the baseline spindle activity.”\nOur initial approach focused on computing and comparing p-values. That’s a bad idea. We’re not as interested in comparing the evidence we have for each null-hypothesis (the p-value); that depends on both the effect size and the number of observations.\nInstead, we’re more interested in comparing the spindle activity between condidtions. In other words, we’re intested in the effect size, not the p-value.\nThis suggests a different analysis path forward for an improved approach. We can answer the same scienfitic question by comparing the spindle activitys beteween conditions, not the p-values. We’ve started to see this in the plots of spindle activity at baseline, during treatment, and post-treatment. For more analysis (e.g., different statistical test and effect size) continue on to other Minis.\n\n5- A different test … a different result.\nOriginally (in Mini 2), we considered the null hypothesis:\n\nNo difference in mean spindle activity during treatment from 0.\n\nLet’s now consider a different null hypothesis:\n\nNo difference in mean spindle activity during treatment versus baseline.\n\nTo test this null hypothesis, we’ll apply a different test:\n\na subject specific two-sample t-test.\n\n\nresult = stats.ttest_ind(baseline, during_treatment, alternative='less')\np_value_baseline_vs_during = result.pvalue\nfor k in np.arange(0,20):\n    print('Subject ', k, ', p=', np.array2string(p_value_baseline_vs_during[k], precision=4))\n\n\nplt.figure(figsize=(12, 2))\nplt.stem(p_value_baseline_vs_during);\nplt.axhline(y=0.05/20, color='r', linestyle='--')\nplt.xlabel('Subject'); plt.ylabel('p value'); plt.title('Baseline versus During Treatment'); plt.yscale('log')\nprint('Significant p-values during treatment = ',np.sum(p_value_baseline_vs_during &lt; 0.05/20))\n\n\nQ: Interpret these p-value results in terms of the null hypothesis.\nA: We conclude that, in most cases, we reject the null hypothesis of no difference in spindle activity between baseline versus during treatment. We conclude that the spindle activity is higher during treatment compared to baseline in most subjects (14/20).\n\n\nQ: Compare the results of these tests for each human to the results in Mini 2.\nA: In Mini 2, we computed p-values during treatment versus 0. Now, we compute p-values during treatment versus baseline. These are different tests. The latter tests more directly our scientific question.\n\n\n\n6- So, what went wrong?\nWhat is the scientifc question we’re actually trying to answer?\n(PENDING): effect-size\n(REVIEW): Two things impact the strength of evidence: - the effect size (i.e., the values of the spindle activity). - the number of observations\n\n\n7- One test to rule them all: an omnibus test.\n(PENDING)\n\n\n8- Beyond p-values: estimate what you care about.\n(PENDING): estimate effect size during & post, and compare.\n\n\n9- Summary"
  },
  {
    "objectID": "OLD/METER_P_Values-01-08-2025.html#a-different-test-a-different-result.",
    "href": "OLD/METER_P_Values-01-08-2025.html#a-different-test-a-different-result.",
    "title": "Putting the p-value in Context",
    "section": "5- A different test … a different result.",
    "text": "5- A different test … a different result.\nOriginally (in Mini 2), we considered the null hypothesis:\n\nNo difference in mean spindle activity during treatment from 0.\n\nLet’s now consider a different null hypothesis:\n\nNo difference in mean spindle activity during treatment versus baseline.\n\nTo test this null hypothesis, we’ll apply a different test:\n\na subject specific two-sample t-test.\n\n\nresult = stats.ttest_ind(baseline, during_treatment, alternative='less')\np_value_baseline_vs_during = result.pvalue\nfor k in np.arange(0,20):\n    print('Subject ', k, ', p=', np.array2string(p_value_baseline_vs_during[k], precision=4))\n\n\nplt.figure(figsize=(12, 2))\nplt.stem(p_value_baseline_vs_during);\nplt.axhline(y=0.05/20, color='r', linestyle='--')\nplt.xlabel('Subject'); plt.ylabel('p value'); plt.title('Baseline versus During Treatment'); plt.yscale('log')\nprint('Significant p-values during treatment = ',np.sum(p_value_baseline_vs_during &lt; 0.05/20))\n\n\nQ: Interpret these p-value results in terms of the null hypothesis.\nA: We conclude that, in most cases, we reject the null hypothesis of no difference in spindle activity between baseline versus during treatment. We conclude that the spindle activity is higher during treatment compared to baseline in most subjects (14/20).\n\n\nQ: Compare the results of these tests for each human to the results in Mini 2.\nA: In Mini 2, we computed p-values during treatment versus 0. Now, we compute p-values during treatment versus baseline. These are different tests. The latter tests more directly our scientific question."
  },
  {
    "objectID": "OLD/METER_P_Values-01-08-2025.html#so-what-went-wrong",
    "href": "OLD/METER_P_Values-01-08-2025.html#so-what-went-wrong",
    "title": "Putting the p-value in Context",
    "section": "6- So, what went wrong?",
    "text": "6- So, what went wrong?\nWhat is the scientifc question we’re actually trying to answer?\n(PENDING): effect-size\n(REVIEW): Two things impact the strength of evidence: - the effect size (i.e., the values of the spindle activity). - the number of observations"
  },
  {
    "objectID": "OLD/METER_P_Values-01-08-2025.html#one-test-to-rule-them-all-an-omnibus-test.",
    "href": "OLD/METER_P_Values-01-08-2025.html#one-test-to-rule-them-all-an-omnibus-test.",
    "title": "Putting the p-value in Context",
    "section": "7- One test to rule them all: an omnibus test.",
    "text": "7- One test to rule them all: an omnibus test.\n(PENDING)"
  },
  {
    "objectID": "OLD/METER_P_Values-01-08-2025.html#beyond-p-values-estimate-what-you-care-about.",
    "href": "OLD/METER_P_Values-01-08-2025.html#beyond-p-values-estimate-what-you-care-about.",
    "title": "Putting the p-value in Context",
    "section": "8- Beyond p-values: estimate what you care about.",
    "text": "8- Beyond p-values: estimate what you care about.\n(PENDING): estimate effect size during & post, and compare."
  },
  {
    "objectID": "OLD/METER_P_Values-01-08-2025.html#summary",
    "href": "OLD/METER_P_Values-01-08-2025.html#summary",
    "title": "Putting the p-value in Context",
    "section": "9- Summary",
    "text": "9- Summary"
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html",
    "href": "METER_Sample_Size-SHORT-Quarto.html",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "",
    "text": "Based on the groundbreaking research previously conducted in your lab, you and your collaborators have formulated a compelling scientific hypothesis: substance \\(x\\) could be a genetic biomarker for longevity, potentially influencing the age at which individuals pass away. This intriguing hypothesis opens up a new frontier in our understanding of genetics and lifespan, promising significant advancements in the field.\nBefore we can embark on an experimental journey to test the predictive power of this novel biomarker, we must first tackle a critical step: determining the appropriate sample size for a follow-up research study. The sample size is not just a number; it is a cornerstone of experimental design that ensures our data will be robust enough to support or refute our hypothesis.\nTo accurately compute this sample size, we need to consider our prior beliefs and existing knowledge about substance \\(x\\) and its relationship to longevity. Let’s delve into the specifics. Imagine we have the following limited yet crucial pieces of information:\n\nDistribution of Substance \\(x\\): The expression levels of substance \\(x\\) in people follow a normal distribution.\nImpact on Longevity: Individuals at the high end of the expression spectrum tend to live approximately 5 years longer than those at the low end.\n\nGiven these insights, our task is to calculate a sample size that can yield statistically significant results. This endeavor will not only help us test our hypothesis with precision but also pave the way for future research that could revolutionize our understanding of genetic influences on lifespan. Let’s proceed with this vital calculation, knowing that the outcomes will bring us one step closer to potentially groundbreaking discoveries in genetic biomarkers and longevity.\n\n\n\n\n\n\nGiven this information, how many individuals should we include in our study to have a reasonable chance of demonstrating this hypothesis is correct? (I.e., What is the sample size?)\n\n\n\n\n\n\n\n\n\n\n\n\nWait, I have no idea how to answer this?\n\n\n\n\nDon’t worry!\nThe goal of this unit is to teach you to tackle this problem.\nLet’s first come up with any approach to compute a sample size, even if we’re not confident in the results.\n\nA few possible places to start:\n\nTake an educated guess: Perhaps you have taken part in or read about similar research before. What order of magnitude seems right for this sort of experiment?\nFind a source: Sample size estimation is a common topic in introductory statistics textbooks. These often include formulas that students can use to compute sample size for specific categories of questions.\nGoogle it: There are many web-based resources (including online calculators) that are designed to enable sample size calculations. Search engines provide a starting point for finding such resource Doing so, you might end up at a website like this or like this.\n\nOr, if you’d like to skip this step, we’ll suggest a sample size of 100.\n\n\n\n\n\n\n\n\nIt you estimated the sample size, what obstacles did you encounter along the way?\n\n\n\n\nSample size calculations aren’t always easy or obvious, even for veteran researchers!\nHere’s a good video of the challenge.\n\n\n\n\n\n\n\n\n\nGiven the description of the scientific hypothesis and experiment, think about what data you would collect and what analyses you would perform to test the hypothesis.\n\n\n\n\nWhat types of values do you expect for each variable? What are their distributions, do you think?\nHow do you expect the variables to be related?\nTry drawing a sketch of what you imagine a successful result might look like?\n(Text) For each participant, we will collect expression levels of substance \\(x\\) and age at death.\n(Text) I expect age at death to increase with \\(x\\).\n(Multiple Choice) Show different plots of \\(x\\) versus age at death, and ask learner to select the plot most consistent with the hypothesis.\n\n\n\n\n\n\n\n\n\nWe provided very little information and asked you to compute the sample size. What other information do you think would be helpful to estimate the sample size?"
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#which-path-will-you-pursue-to-compute-the-sample-size",
    "href": "METER_Sample_Size-SHORT-Quarto.html#which-path-will-you-pursue-to-compute-the-sample-size",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "Which path will you pursue to compute the sample size?",
    "text": "Which path will you pursue to compute the sample size?"
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#a--ill-resample-the-data-turn-to-page-3a.",
    "href": "METER_Sample_Size-SHORT-Quarto.html#a--ill-resample-the-data-turn-to-page-3a.",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "3A- I’ll resample the data, Turn To Page 3A.",
    "text": "3A- I’ll resample the data, Turn To Page 3A."
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#b--ill-build-models-using-the-data-turn-to-page-3b.",
    "href": "METER_Sample_Size-SHORT-Quarto.html#b--ill-build-models-using-the-data-turn-to-page-3b.",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "3B- I’ll build models using the data, Turn To Page 3B.",
    "text": "3B- I’ll build models using the data, Turn To Page 3B."
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#c--ill-build-models-without-the-data-turn-to-page-3c.",
    "href": "METER_Sample_Size-SHORT-Quarto.html#c--ill-build-models-without-the-data-turn-to-page-3c.",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "3C- I’ll build models without the data, Turn To Page 3C.",
    "text": "3C- I’ll build models without the data, Turn To Page 3C."
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#d--ill-use-my-current-sample-size-choice-turn-to-page-3d.",
    "href": "METER_Sample_Size-SHORT-Quarto.html#d--ill-use-my-current-sample-size-choice-turn-to-page-3d.",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "3D- I’ll use my current sample size choice, Turn to Page 3D.",
    "text": "3D- I’ll use my current sample size choice, Turn to Page 3D."
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#turn-to-page-4-summary",
    "href": "METER_Sample_Size-SHORT-Quarto.html#turn-to-page-4-summary",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "Turn to Page 4 Summary",
    "text": "Turn to Page 4 Summary"
  },
  {
    "objectID": "METER_Sample_Size-SHORT-Quarto.html#turn-to-page-4-summary.",
    "href": "METER_Sample_Size-SHORT-Quarto.html#turn-to-page-4-summary.",
    "title": "Sample Size - How much data is enough for your experiment?",
    "section": "Turn to Page 4 Summary.",
    "text": "Turn to Page 4 Summary."
  }
]